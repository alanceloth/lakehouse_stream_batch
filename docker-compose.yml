version: "3.9"

networks:
  bigdata_net:
    driver: bridge        # todos no mesmo bridge network ⇒ DNS interno

volumes:
  pgdata:

services:
# ---------------------- Zookeeper -------------------------------
  zookeeper:
    image: bitnami/zookeeper:3.9
    container_name: zookeeper
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks: [bigdata_net]

# ---------------------- Kafka -----------------------------------
  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    depends_on: [zookeeper]
    networks: [bigdata_net]
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      # uma porta para acesso interno (kafka:9092) e outra externa (localhost:19092)
      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:19092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:19092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
    ports:
      - "19092:19092"     # publica só a porta externa

# ---------------------- Flink cluster ---------------------------
  jobmanager:
    image: flink:2.0.0-scala_2.12-java11
    container_name: jobmanager
    networks: [bigdata_net]
    ports:
      - "8081:8081"       # Flink Dashboard
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager

  taskmanager:
    image: flink:2.0.0-scala_2.12-java11
    container_name: taskmanager
    networks: [bigdata_net]
    depends_on: [jobmanager]
    command: taskmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      - TASKMANAGER_NUMBER_OF_TASK_SLOTS=4

# ---------------------- Spark cluster ---------------------------
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    networks: [bigdata_net]
    environment:
      - SPARK_MODE=master
    ports:
      - "8080:8080"       # Spark Master UI

  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark-worker
    networks: [bigdata_net]
    depends_on: [spark-master]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_WEBUI_PORT=8082
    ports:
      - "8082:8082"       # Spark Worker UI

# ---------------------- PostgreSQL ------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    networks: [bigdata_net]
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./monitoring/grafana-provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./init_db.sql:/docker-entrypoint-initdb.d/init_db.sql 

# ---------------------- API ------------------------------
  api:
    build: ./api        # veremos o Dockerfile
    container_name: api
    depends_on: [kafka]
    environment:
      - KAFKA_BOOTSTRAP=${KAFKA_BOOTSTRAP}
      - BATCH_DIR=${BATCH_DIR}
    volumes:
      - ./data:/data            # onde ficarão os arquivos Parquet
    ports:
      - "8000:8000"
    networks: [bigdata_net]

# ---------------- Prometheus -----------------------------------
  prometheus:
    image: prom/prometheus:v2.52
    container_name: prometheus
    networks: [bigdata_net]
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

# ---------------- Grafana --------------------------------------
  grafana:
    image: grafana/grafana:11.0.0
    container_name: grafana
    networks: [bigdata_net]
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    ports:
      - "3000:3000"
    depends_on: [prometheus]
    volumes:
      - ./monitoring/grafana-provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana-provisioning/dashboards:/etc/grafana/provisioning/dashboards

# ---------------- Kafka‑UI -------------------------------------
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    networks: [bigdata_net]
    depends_on: [kafka]
    environment:
      - KAFKA_CLUSTERS_0_NAME=demo
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
    ports:
      - "8085:8080"       # externo 8085 → interno 8080

# ---------------- JMX exporters --------------------------------
  kafka-jmx-exporter:
    image: bitnami/jmx-exporter:0.20.0
    container_name: kafka-jmx-exporter
    networks: [bigdata_net]
    restart: unless-stopped
    command: >
      --config=/config/kafka.yml
      --web.listen-address=:9404
      --web.telemetry-path=/metrics
      kafka:9999
    volumes:
      - ./monitoring/kafka-jmx.yml:/config/kafka.yml

  spark-jmx-exporter:
    image: bitnami/jmx-exporter:0.20.0
    container_name: spark-jmx-exporter
    networks: [bigdata_net]
    command: >
      --config=/config/spark.yml
      --web.listen-address=:9405
      spark-master:7071
    volumes:
      - ./monitoring/spark-jmx.yml:/config/spark.yml


  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: postgres-exporter
    networks: [bigdata_net]
    environment:
      DATA_SOURCE_NAME: "postgresql://admin:admin@postgres:5432/demo?sslmode=disable"
    depends_on: [postgres]
    ports: ["9187:9187"]   # opcional ─ externo
